# -*- coding: utf-8 -*-
"""CNN f1 as 0.94 0.95

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y7T_OTOWr6o8feU0uK_K5BTs2bk8OqJY

# Malaria Classification using CNN

## This work uses CNN to classify images of cells either uninfected or infected with Malaria. The F1 score of both classess are 0.94 and 0.95.
"""

import os
import pandas as pd
import numpy as np
import seaborn as sns
import PIL
import matplotlib.pyplot as plt
from matplotlib.image import imread

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import pathlib
path = "/kaggle/input/cell-images-for-detecting-malaria/cell_images/cell_images"
data_dir = pathlib.Path(path).with_suffix('')

image_count = len(list(data_dir.glob('*/*.png')))
print(image_count)

uninfected = list(data_dir.glob("Uninfected/*"))
parasitized = list(data_dir.glob("Parasitized/*"))
print("Number of uninfected cells", len(uninfected))
print("Number of infected cells", len(parasitized))

PIL.Image.open(uninfected[1])

PIL.Image.open(parasitized[1])

from PIL import Image

def get_image_dimensions(image_path):
    with Image.open(image_path) as img:
        return img.size

def check_image_dimensions(directory):
    # List all files in the directory
    files = os.listdir(directory)

    # Filter out only image files
    image_files = [file for file in files if file.endswith(('jpg', 'jpeg', 'png', 'gif'))]

    # Iterate through each image file and print its dimensions
    for image_file in image_files:
        image_path = os.path.join(directory, image_file)
        dimensions = get_image_dimensions(image_path)
        print(f"Image: {image_file}, Dimensions: {dimensions}")

data_dir

path

Uninfected_path = f'{path}/Uninfected/'
Infected_path = f'{path}/Parasitized/'

dim1 = []
dim2 = []
for image_filename in os.listdir(Uninfected_path):

    if not image_filename.endswith(('jpg', 'jpeg', 'png', 'gif')):
                         continue

    img = imread(Uninfected_path+image_filename)

    d1,d2,colors = img.shape
    dim1.append(d1)
    dim2.append(d2)

sns.jointplot(x=dim1, y=dim2, kind="scatter")
plt.show()

# mean of dimension of uninfected images
np.mean(dim1), np.mean(dim2)

dim1 = []
dim2 = []
for image_filename in os.listdir(Infected_path):

    if not image_filename.endswith(('jpg', 'jpeg', 'png', 'gif')):
                         continue

    img = imread(Infected_path+image_filename)

    d1,d2,colors = img.shape
    dim1.append(d1)
    dim2.append(d2)

sns.jointplot(x=dim1, y=dim2, kind="scatter")
plt.show()

# mean of dimension of Infected images
np.mean(dim1), np.mean(dim2)

# image size of both set of images is around 130 x 130
image_shape = (130,130,3)

from PIL import Image

image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees
                               width_shift_range=0.10, # randomly shift the pic width by a max of 5%
                               height_shift_range=0.10, # randonly shift the pic height by a max of 5%
                               # rescale=1/255, # Rescale the image by normalzing it.
                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)
                               zoom_range=0.1, # Zoom in by 10% max
                               horizontal_flip=True, # Allo horizontal flipping
                               fill_mode='nearest', # Fill in missing pixels with the nearest filled value
                               validation_split = 0.3,
                              )

"""### Testing ImageDataGenerator"""

para_img = imread(parasitized[1])
plt.imshow(para_img)

plt.imshow(image_gen.random_transform(para_img))

"""## Generating Manipulated Images from a Directory"""

batch_size = 16

training_data = image_gen.flow_from_directory(
    data_dir,
    subset = "training",
    class_mode="binary",
    target_size = image_shape[:2],
    batch_size = batch_size
)

validation_data = image_gen.flow_from_directory(
    data_dir,
    subset = "validation",
    class_mode="binary",
    target_size = image_shape[:2],
    batch_size = batch_size,
    shuffle=False
)

training_data.class_indices

validation_data.class_indices

"""## Creating the Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D

model = Sequential()

model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu',))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())

model.add(Dense(128))
model.add(Activation('relu'))

# Dropouts help reduce overfitting by randomly turning neurons off during training.
# Here we say randomly turn off 50% of neurons.
model.add(Dropout(0.5))

# Last layer, remember its binary so we use sigmoid
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

"""## Early Stopping"""

from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss',patience=3)

"""## Training the Model"""

import warnings
warnings.filterwarnings('ignore')

history = model.fit(training_data, epochs=25, validation_data=validation_data, callbacks=[early_stop])

"""## Evaluating the Model"""

losses = pd.DataFrame(model.history.history)

losses[['loss','val_loss']].plot()

model.evaluate(validation_data)

pred_probabilities = model.predict(validation_data).flatten()

predictions = (pred_probabilities > 0.5).astype(np.int32)

from sklearn.metrics import classification_report,confusion_matrix
confusion_matrix(validation_data.classes,predictions)

print(classification_report(validation_data.classes,predictions))

